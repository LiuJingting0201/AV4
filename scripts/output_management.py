#!/usr/bin/env python3
"""
TruckScenes è¾“å‡ºç®¡ç†æ¨¡å—
åŠŸèƒ½ï¼š
1. æ‰¹é‡å¤„ç†é€‰å®šçš„æ ·æœ¬
2. ä¿å­˜å¯è§†åŒ–å›¾ç‰‡ã€ç‚¹äº‘æ•°æ®ã€ç»Ÿè®¡ä¿¡æ¯
3. ç”Ÿæˆæ±‡æ€»æŠ¥å‘Šå’ŒCSVæ–‡ä»¶
4. é”™è¯¯å¤„ç†å’Œæ—¥å¿—è®°å½•
"""

import os
import json
import traceback
import pandas as pd
import numpy as np
import inspect
from datetime import datetime
import matplotlib.pyplot as plt

# å°è¯•å¯¼å…¥open3dï¼ˆç”¨äºä¿å­˜ç‚¹äº‘ï¼‰
try:
    import open3d as o3d
    O3D_AVAILABLE = True
    print("âœ“ Open3D available for point cloud saving")
except ImportError:
    O3D_AVAILABLE = False
    print("âš ï¸ Open3D not available - point clouds will not be saved")

# å¯¼å…¥ä¸»å¯è§†åŒ–ç±»
try:
    from visualize_fused_pts_and_3d_boxes import TruckScenesVisualizer
    print("âœ“ TruckScenesVisualizer imported successfully")
except ImportError as e:
    print(f"âŒ Failed to import TruckScenesVisualizer: {e}")
    print("è¯·ç¡®ä¿ visualize_fused_pts_and_3d_boxes.py åœ¨åŒä¸€ç›®å½•ä¸‹")


# ==== 1. è·¯å¾„ä¸é…ç½® ====
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
DATA_DIR = os.path.join(BASE_DIR, "data", "man-truckscenes")
CONFIGS_DIR = os.path.join(BASE_DIR, "configs")
OUTPUT_ROOT = os.path.join(BASE_DIR, "output")
SELECTED_JSON = os.path.join(CONFIGS_DIR, "selected_samples.json")
SUMMARY_DIR = os.path.join(OUTPUT_ROOT, "summary")

# ç¡®ä¿ç›®å½•å­˜åœ¨
os.makedirs(SUMMARY_DIR, exist_ok=True)
os.makedirs(CONFIGS_DIR, exist_ok=True)

print(f"ğŸ“ é…ç½®è·¯å¾„:")
print(f"  - æ•°æ®ç›®å½•: {DATA_DIR}")
print(f"  - é…ç½®ç›®å½•: {CONFIGS_DIR}")
print(f"  - è¾“å‡ºç›®å½•: {OUTPUT_ROOT}")
print(f"  - é€‰æ‹©æ ·æœ¬: {SELECTED_JSON}")


# ==== 2. å·¥å…·å‡½æ•° ====
def ensure_dir(path):
    """ç¡®ä¿ç›®å½•å­˜åœ¨"""
    os.makedirs(path, exist_ok=True)


def save_point_cloud(points, filename):
    """ä¿å­˜ç‚¹äº‘æ•°æ®ä¸ºPLYæ ¼å¼"""
    if points is None or len(points) == 0:
        print(f"âš ï¸ Warning: No points to save for {filename}")
        return False
    
    if not O3D_AVAILABLE:
        print(f"âš ï¸ Warning: Cannot save {filename} - open3d not available")
        return False
        
    try:
        pcd = o3d.geometry.PointCloud()
        pcd.points = o3d.utility.Vector3dVector(points)
        o3d.io.write_point_cloud(filename, pcd)
        print(f"âœ“ Saved point cloud: {os.path.basename(filename)} ({len(points):,} points)")
        return True
    except Exception as e:
        print(f"âŒ Error saving point cloud {filename}: {e}")
        return False


def save_stats(stats, filename):
    """ä¿å­˜ç»Ÿè®¡ä¿¡æ¯ä¸ºJSONæ ¼å¼"""
    try:
        # ç¡®ä¿æ‰€æœ‰numpyæ•°ç»„éƒ½è½¬æ¢ä¸ºPythonåŸç”Ÿç±»å‹
        cleaned_stats = {}
        for key, value in stats.items():
            if isinstance(value, np.ndarray):
                cleaned_stats[key] = value.tolist()
            elif isinstance(value, (np.integer, np.floating)):
                cleaned_stats[key] = value.item()
            else:
                cleaned_stats[key] = value
        
        # æ·»åŠ å¤„ç†æ—¶é—´æˆ³
        cleaned_stats['processed_at'] = datetime.now().isoformat()
                
        with open(filename, "w", encoding="utf-8") as f:
            json.dump(cleaned_stats, f, indent=2, ensure_ascii=False)
        print(f"âœ“ Saved stats: {os.path.basename(filename)}")
        return True
    except Exception as e:
        print(f"âŒ Error saving stats {filename}: {e}")
        return False


def save_annotations(annotations, filename):
    """ä¿å­˜æ ‡æ³¨ä¿¡æ¯ä¸ºJSONæ ¼å¼"""
    try:
        # æ¸…ç†annotationsä¸­çš„numpyæ•°ç»„
        cleaned_annotations = []
        for ann in annotations:
            cleaned_ann = {}
            for key, value in ann.items():
                if isinstance(value, np.ndarray):
                    cleaned_ann[key] = value.tolist()
                elif isinstance(value, (np.integer, np.floating)):
                    cleaned_ann[key] = value.item()
                else:
                    cleaned_ann[key] = value
            cleaned_annotations.append(cleaned_ann)
            
        with open(filename, "w", encoding="utf-8") as f:
            json.dump(cleaned_annotations, f, indent=2, ensure_ascii=False)
        print(f"âœ“ Saved annotations: {os.path.basename(filename)} ({len(annotations)} boxes)")
        return True
    except Exception as e:
        print(f"âŒ Error saving annotations {filename}: {e}")
        return False


def save_figure(fig, filename, dpi=150):
    """ä¿å­˜matplotlibå›¾ç‰‡"""
    try:
        fig.savefig(filename, dpi=dpi, bbox_inches='tight', 
                   facecolor='white', edgecolor='none')
        plt.close(fig)  # å…³é—­å›¾å½¢ä»¥é‡Šæ”¾å†…å­˜
        print(f"âœ“ Saved visualization: {os.path.basename(filename)}")
        return True
    except Exception as e:
        print(f"âŒ Error saving figure {filename}: {e}")
        return False


def log_error(error_msg, log_path):
    """è®°å½•é”™è¯¯æ—¥å¿—"""
    try:
        ensure_dir(os.path.dirname(log_path))
        with open(log_path, "a", encoding="utf-8") as f:
            f.write(f"\n{'='*50}\n")
            f.write(f"Timestamp: {datetime.now().isoformat()}\n")
            f.write(f"Error: {error_msg}\n")
            f.write(f"{'='*50}\n")
    except Exception as e:
        print(f"âŒ Failed to write error log: {e}")


# ==== 3. ä¸»æ‰¹é‡å¤„ç†å‡½æ•° ====
def process_sample(visualizer, scene_name, sample_token, output_root, max_distance=50.0):
    """
    å¤„ç†å•ä¸ªæ ·æœ¬
    
    Args:
        visualizer: TruckScenesVisualizerå®ä¾‹
        scene_name: åœºæ™¯åç§°
        sample_token: æ ·æœ¬token
        output_root: è¾“å‡ºæ ¹ç›®å½•
        max_distance: æœ€å¤§æ˜¾ç¤ºè·ç¦»
    
    Returns:
        dict: ç»Ÿè®¡ä¿¡æ¯ï¼Œå¤±è´¥æ—¶è¿”å›None
    """
    try:
        print(f"\nğŸ”„ Processing: {scene_name} | {sample_token[:16]}...")
        
        # æ£€æŸ¥ visualize_sample æ–¹æ³•æ˜¯å¦æ”¯æŒ return_data å‚æ•°
        import inspect
        sig = inspect.signature(visualizer.visualize_sample)
        
        if 'return_data' in sig.parameters:
            # æ–°ç‰ˆæœ¬ï¼šæ”¯æŒ return_data å‚æ•°
            result = visualizer.visualize_sample(
                sample_token, 
                max_distance=max_distance, 
                return_data=True
            )
        else:
            # æ—§ç‰ˆæœ¬ï¼šæ‰‹åŠ¨æ„å»ºç»“æœæ•°æ®
            print("âš ï¸ ä½¿ç”¨å…¼å®¹æ¨¡å¼å¤„ç†æ ·æœ¬...")
            
            # å…ˆè·å–æ ·æœ¬ä¿¡æ¯
            sample = visualizer.ts.get('sample', sample_token)
            
            # æ”¶é›†ä¼ æ„Ÿå™¨æ•°æ®
            available_sensors = list(sample['data'].keys())
            lidar_sensors = [s for s in available_sensors if 'LIDAR' in s.upper()]
            radar_sensors = [s for s in available_sensors if 'RADAR' in s.upper()]
            
            # æ”¶é›†ç‚¹äº‘æ•°æ®
            all_lidar_points = []
            all_radar_points = []
            
            # å¤„ç†LiDARæ•°æ®
            for lidar_sensor in lidar_sensors:
                lidar_token = sample['data'][lidar_sensor]
                lidar_points, _ = visualizer._get_sensor_data(lidar_token)
                if lidar_points is not None:
                    all_lidar_points.append(lidar_points)
            
            # å¤„ç†Radaræ•°æ®  
            for radar_sensor in radar_sensors:
                radar_token = sample['data'][radar_sensor]
                radar_points, _ = visualizer._get_sensor_data(radar_token)
                if radar_points is not None:
                    all_radar_points.append(radar_points)
            
            # åˆå¹¶ç‚¹äº‘æ•°æ®
            merged_lidar = np.vstack(all_lidar_points) if all_lidar_points else np.empty((0, 3))
            merged_radar = np.vstack(all_radar_points) if all_radar_points else np.empty((0, 3))
            
            # è·å–æ ‡æ³¨
            annotations = visualizer._get_sample_annotations(sample_token)
            
            # åˆ›å»ºå¯è§†åŒ–ï¼ˆä½†ä¸æ˜¾ç¤ºï¼‰
            plt.ioff()  # å…³é—­äº¤äº’æ¨¡å¼
            fig, lidar_count, radar_count, box_count = visualizer._create_visualization(
                merged_lidar, merged_radar, annotations, 
                sample_token, max_distance, len(lidar_sensors), len(radar_sensors)
            )
            plt.ion()  # é‡æ–°å¼€å¯äº¤äº’æ¨¡å¼
            
            # æ‰‹åŠ¨æ„å»ºç»“æœ
            result = {
                "fig": fig,
                "merged_lidar": merged_lidar,
                "merged_radar": merged_radar,
                "annotations": annotations,
                "stats": {
                    "sample_token": sample_token,
                    "n_lidar": len(lidar_sensors),
                    "n_radar": len(radar_sensors),
                    "lidar_points": int(len(merged_lidar)),
                    "radar_points": int(len(merged_radar)),
                    "box_count": int(len(annotations)) if annotations else 0,
                    "max_distance": max_distance
                },
                "log": []
            }
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        out_dir = os.path.join(output_root, scene_name, sample_token)
        ensure_dir(out_dir)
        
        # ä¿å­˜å¯è§†åŒ–å›¾ç‰‡
        if result.get("fig"):
            img_path = os.path.join(out_dir, "visualization.png")
            save_figure(result["fig"], img_path)
        
        # ä¿å­˜ç‚¹äº‘æ•°æ®
        if result.get("merged_lidar") is not None:
            lidar_path = os.path.join(out_dir, "merged_lidar.ply")
            if save_point_cloud(result["merged_lidar"], lidar_path):
                result["stats"]["lidar_path"] = lidar_path
        
        if result.get("merged_radar") is not None:
            radar_path = os.path.join(out_dir, "merged_radar.ply")
            if save_point_cloud(result["merged_radar"], radar_path):
                result["stats"]["radar_path"] = radar_path
        
        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        stats_path = os.path.join(out_dir, "stats.json")
        save_stats(result["stats"], stats_path)
        
        # ä¿å­˜æ ‡æ³¨ä¿¡æ¯
        if result.get("annotations"):
            ann_path = os.path.join(out_dir, "annotations.json")
            save_annotations(result["annotations"], ann_path)
        
        # ä¿å­˜å¤„ç†æ—¥å¿—
        if result.get("log"):
            log_path = os.path.join(out_dir, "process_log.txt")
            with open(log_path, "w", encoding="utf-8") as f:
                for log_entry in result["log"]:
                    f.write(f"{log_entry}\n")
        
        print(f"âœ… Successfully processed: {scene_name} | {sample_token[:16]}")
        return result["stats"]
        
    except Exception as e:
        error_msg = f"Error processing {scene_name} | {sample_token}: {str(e)}"
        print(f"âŒ {error_msg}")
        
        # è®°å½•è¯¦ç»†é”™è¯¯ä¿¡æ¯
        error_log_path = os.path.join(output_root, scene_name, sample_token, "error_log.txt")
        ensure_dir(os.path.dirname(error_log_path))
        with open(error_log_path, "w", encoding="utf-8") as f:
            f.write(f"Error processing sample: {sample_token}\n")
            f.write(f"Scene: {scene_name}\n")
            f.write(f"Timestamp: {datetime.now().isoformat()}\n")
            f.write(f"Error message: {str(e)}\n\n")
            f.write("Full traceback:\n")
            f.write(traceback.format_exc())
        
        return None


def create_sample_config():
    """åˆ›å»ºç¤ºä¾‹é…ç½®æ–‡ä»¶"""
    sample_config = {
        "scene-0001": [
            "32d2bcf46e734dffb14fe2e0a823d059",
            "e3b2c1d4a5f6789012345678901234ab"
        ],
        "scene-0002": [
            "f4c3b2a1d5e6789012345678901234cd",
            "a1b2c3d4e5f6789012345678901234ef"
        ]
    }
    
    with open(SELECTED_JSON, "w", encoding="utf-8") as f:
        json.dump(sample_config, f, indent=2)
    print(f"âœ“ Created sample config: {SELECTED_JSON}")


def generate_summary_report(all_stats, summary_dir):
    """ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š"""
    if not all_stats:
        print("âš ï¸ No data available for summary report")
        return
    
    df = pd.DataFrame(all_stats)
    
    # ä¿å­˜å®Œæ•´ç»Ÿè®¡CSV
    csv_path = os.path.join(summary_dir, "all_stats.csv")
    df.to_csv(csv_path, index=False)
    print(f"âœ“ Saved complete stats: {csv_path}")
    
    # ç”Ÿæˆæ±‡æ€»ç»Ÿè®¡
    summary_stats = {
        "total_samples": len(df),
        "total_scenes": df['scene_name'].nunique() if 'scene_name' in df.columns else 0,
        "total_lidar_points": int(df['lidar_points'].sum()) if 'lidar_points' in df.columns else 0,
        "total_radar_points": int(df['radar_points'].sum()) if 'radar_points' in df.columns else 0,
        "total_boxes": int(df['box_count'].sum()) if 'box_count' in df.columns else 0,
        "avg_lidar_points": float(df['lidar_points'].mean()) if 'lidar_points' in df.columns else 0,
        "avg_radar_points": float(df['radar_points'].mean()) if 'radar_points' in df.columns else 0,
        "avg_boxes": float(df['box_count'].mean()) if 'box_count' in df.columns else 0,
        "processing_date": datetime.now().isoformat()
    }
    
    # ä¿å­˜æ±‡æ€»ç»Ÿè®¡
    summary_path = os.path.join(summary_dir, "summary_stats.json")
    with open(summary_path, "w", encoding="utf-8") as f:
        json.dump(summary_stats, f, indent=2)
    print(f"âœ“ Saved summary stats: {summary_path}")
    
    # æŒ‰åœºæ™¯åˆ†ç»„ç»Ÿè®¡
    if 'scene_name' in df.columns:
        scene_stats = df.groupby('scene_name').agg({
            'lidar_points': ['count', 'sum', 'mean'] if 'lidar_points' in df.columns else ['count'],
            'radar_points': ['sum', 'mean'] if 'radar_points' in df.columns else [],
            'box_count': ['sum', 'mean'] if 'box_count' in df.columns else []
        }).round(2)
        
        scene_csv_path = os.path.join(summary_dir, "scene_summary.csv")
        scene_stats.to_csv(scene_csv_path)
        print(f"âœ“ Saved scene summary: {scene_csv_path}")
    
    print(f"\nğŸ“Š å¤„ç†æ±‡æ€»:")
    print(f"  - æ€»æ ·æœ¬æ•°: {summary_stats['total_samples']}")
    print(f"  - æ€»åœºæ™¯æ•°: {summary_stats['total_scenes']}")
    print(f"  - æ€»LiDARç‚¹: {summary_stats['total_lidar_points']:,}")
    print(f"  - æ€»Radarç‚¹: {summary_stats['total_radar_points']:,}")
    print(f"  - æ€»æ ‡æ³¨æ¡†: {summary_stats['total_boxes']}")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš› TruckScenes æ‰¹é‡è¾“å‡ºç®¡ç†ç³»ç»Ÿ")
    print("=" * 50)
    
    # ==== 4. æ£€æŸ¥é…ç½®æ–‡ä»¶ ====
    if not os.path.exists(SELECTED_JSON):
        print(f"âš ï¸ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {SELECTED_JSON}")
        print("ğŸ”§ åˆ›å»ºç¤ºä¾‹é…ç½®æ–‡ä»¶...")
        create_sample_config()
        print("ğŸ“ è¯·ç¼–è¾‘é…ç½®æ–‡ä»¶ï¼Œæ·»åŠ è¦å¤„ç†çš„åœºæ™¯å’Œæ ·æœ¬token")
        return
    
    # åŠ è½½é…ç½®
    try:
        with open(SELECTED_JSON, "r", encoding="utf-8") as f:
            scene_samples = json.load(f)
        print(f"âœ“ åŠ è½½é…ç½®æ–‡ä»¶: {len(scene_samples)} ä¸ªåœºæ™¯")
    except Exception as e:
        print(f"âŒ æ— æ³•åŠ è½½é…ç½®æ–‡ä»¶: {e}")
        return
    
    # ==== 5. åˆå§‹åŒ–å¯è§†åŒ–å·¥å…· ====
    try:
        print("ğŸ”§ åˆå§‹åŒ– TruckScenes å¯è§†åŒ–å·¥å…·...")
        visualizer = TruckScenesVisualizer(DATA_DIR, version="v1.0-mini")
        print("âœ“ å¯è§†åŒ–å·¥å…·åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        print(f"âŒ æ— æ³•åˆå§‹åŒ–å¯è§†åŒ–å·¥å…·: {e}")
        return
    
    # ==== 6. æ‰¹é‡å¤„ç† ====
    all_stats = []
    total_samples = sum(len(samples) for samples in scene_samples.values())
    processed_count = 0
    
    print(f"\nğŸš€ å¼€å§‹æ‰¹é‡å¤„ç† {total_samples} ä¸ªæ ·æœ¬...")
    
    for scene_name, sample_list in scene_samples.items():
        print(f"\nğŸ“‚ å¤„ç†åœºæ™¯: {scene_name} ({len(sample_list)} ä¸ªæ ·æœ¬)")
        
        for sample_token in sample_list:
            processed_count += 1
            print(f"[{processed_count}/{total_samples}] ", end="")
            
            stats = process_sample(
                visualizer, scene_name, sample_token, OUTPUT_ROOT
            )
            
            if stats is not None:
                stats["scene_name"] = scene_name
                stats["sample_token"] = sample_token
                all_stats.append(stats)
    
    # ==== 7. ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š ====
    print(f"\nğŸ“ˆ ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š...")
    generate_summary_report(all_stats, SUMMARY_DIR)
    
    # ==== 8. å®Œæˆä¿¡æ¯ ====
    success_count = len(all_stats)
    failure_count = total_samples - success_count
    
    print(f"\nğŸ‰ æ‰¹é‡å¤„ç†å®Œæ¯•!")
    print(f"  âœ… æˆåŠŸ: {success_count}/{total_samples}")
    if failure_count > 0:
        print(f"  âŒ å¤±è´¥: {failure_count}/{total_samples}")
    print(f"  ğŸ“ è¾“å‡ºç›®å½•: {OUTPUT_ROOT}")
    print(f"  ğŸ“Š æ±‡æ€»æŠ¥å‘Š: {SUMMARY_DIR}")
    
    if success_count == 0:
        print("\nâš ï¸ æ²¡æœ‰æˆåŠŸå¤„ç†çš„æ ·æœ¬ï¼Œè¯·æ£€æŸ¥:")
        print("  1. æ•°æ®è·¯å¾„æ˜¯å¦æ­£ç¡®")
        print("  2. æ ·æœ¬tokenæ˜¯å¦æœ‰æ•ˆ")
        print("  3. ä¾èµ–åŒ…æ˜¯å¦æ­£ç¡®å®‰è£…")


if __name__ == "__main__":
    main()